{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNP0ES30bkWyRNh3UsG0Swj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kulraj97/campusx/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nD42J5hOf1wj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **side by side bar graph**\n",
        " "
      ],
      "metadata": {
        "id": "GFdAfOF-f5mK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/reviews.zip\")\n",
        "df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "pvmZdCmevnVD",
        "outputId": "81fce3b4-45b7-468b-e76f-98adfa86839c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-52b91a52-1653-4eb7-b7c6-24a86b8845bf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-52b91a52-1653-4eb7-b7c6-24a86b8845bf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-52b91a52-1653-4eb7-b7c6-24a86b8845bf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-52b91a52-1653-4eb7-b7c6-24a86b8845bf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"review\"]=df[\"review\"].str.lower()"
      ],
      "metadata": {
        "id": "bIz544JOA_Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#REMOVING HTML TAGS**"
      ],
      "metadata": {
        "id": "RQUB3Gw6B4FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_tags(text):\n",
        "    return re.sub('<[^<]+?>', '', text)\n",
        "\n",
        "df['review'] = df['review'].apply(remove_tags)"
      ],
      "metadata": {
        "id": "5MIOgIYfBGOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CGUnv0ByBOPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REMOVING URLS**"
      ],
      "metadata": {
        "id": "ECGaEyKZDm6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_urls(text):\n",
        "    return re.sub(r'https?://\\S+', '', text)\n",
        "\n",
        "df['column_name'] = df['column_name'].apply(remove_urls)"
      ],
      "metadata": {
        "id": "ba1bvOSvBqwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RMOVING PUNCTUATION**"
      ],
      "metadata": {
        "id": "2tmKMFjDENmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "df['column_name'] = df['column_name'].apply(remove_punctuation)"
      ],
      "metadata": {
        "id": "X5Wso99TEL_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "df['column_name'] = df['column_name'].apply(remove_punctuation)"
      ],
      "metadata": {
        "id": "hp8yBOlOEM1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SLANG WORD TREATMENT**"
      ],
      "metadata": {
        "id": "Hko-7uT1GaXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def convert_slang(text):\n",
        "    # Define a dictionary of slang words and their expanded form\n",
        "    slang_dict = {\n",
        "        'wyd': 'what are you doing',\n",
        "        'btw': 'by the way',\n",
        "        'brb': 'be right back',\n",
        "        'lmao': 'laughing my ass off',\n",
        "        'omg': 'oh my god',\n",
        "        'rofl': 'rolling on the floor laughing'\n",
        "    }\n",
        "    \n",
        "    # Use a regular expression to find slang words in the text\n",
        "    for slang, expanded in slang_dict.items():\n",
        "        text = re.sub(r'\\b' + slang + r'\\b', expanded, text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "df['column_name'] = df['column_name'].apply(convert_slang)"
      ],
      "metadata": {
        "id": "Ew0HnjDIGZjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1tf9YQMJGaMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPELLING CORRECTION**"
      ],
      "metadata": {
        "id": "KKLT9plTHHOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " pip install pyspellchecker\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoVkztmSHcLd",
        "outputId": "1261a816-b2bb-4111-85d1-8044b7021d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textblob\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsDpRK9IIhBN",
        "outputId": "e6e3f516-b298-40b3-f64f-bda3f7a78aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.8/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.8/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def correct_spelling(text):\n",
        "    return str(TextBlob(text).correct())\n",
        "\n",
        "df['column_name'] = df['column_name'].apply(correct_spelling)"
      ],
      "metadata": {
        "id": "lb6HZUGlGe-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REMOVING STOP WORDS(EXCEPT PARTS OF SPEECH AND TAGGING)**"
      ],
      "metadata": {
        "id": "3Mz86YAUJ4T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the stopwords list from nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Import the stopwords list\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # Split the text into words\n",
        "    words = text.split()\n",
        "    \n",
        "    # Remove the stopwords\n",
        "    filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
        "    \n",
        "    # Join the words back into a single string\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "df['column_name'] = df['column_name'].apply(remove_stopwords)\n"
      ],
      "metadata": {
        "id": "oA1xoUgRJ_YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HANDLING EMOJIS**"
      ],
      "metadata": {
        "id": "Zi00HCg7Klgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## REMOVING EMOJIS\n",
        "\n",
        "import re\n",
        "\n",
        "def remove_emojis(text):\n",
        "    return re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "\n",
        "#df['column_name'] = df['column_name'].apply(remove_emojis)\n",
        "remove_emojis(\"hii 😀\")"
      ],
      "metadata": {
        "id": "QGfz-uhFKlUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##REOLACE EMOJIS\n",
        "\n",
        "import emoji\n",
        "def remove_emojis(text):\n",
        "    return emoji.demojize(text)\n",
        "\n",
        "df['column_name'] = df['column_name'].apply(remove_emojis)"
      ],
      "metadata": {
        "id": "Vo5K31viLnVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOKENIZATION**"
      ],
      "metadata": {
        "id": "rBxc6NsM6y3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##NLTK LIB\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "df['tokens'] = df['text'].apply(tokenize)\n"
      ],
      "metadata": {
        "id": "wf_NKw7q6yCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##spacy lib (better results)\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.text for token in doc]\n",
        "\n",
        "df['tokens'] = df['text'].apply(tokenize_spacy)\n",
        "\n"
      ],
      "metadata": {
        "id": "u07qIaqf6x_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LEMMETIZATION/STEMMING**"
      ],
      "metadata": {
        "id": "3k5oEXip8BbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BOTH DO SAME WORK,........ stemming can sometimes produce non-existent or unconventional words, so it is not always the best choice for text processing tasks. \n",
        "#In some cases, it may be better to use lemmatization, which reduces words to their base forms while taking into account the part of speech and the context in which the word is used."
      ],
      "metadata": {
        "id": "LaMIBxHo7Elg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def stem_nltk(words):\n",
        "    stemmed_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return stemmed_words\n",
        "\n",
        "df['stemmed_words'] = df['words'].apply(stem_nltk)"
      ],
      "metadata": {
        "id": "VmICHe4U7EjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEXT VECTORIZATION**"
      ],
      "metadata": {
        "id": "QVaVfNTMVm4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#BAG OF WORDS\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Toy dataset\n",
        "data = [\"This is a sentence.\", \n",
        "        \"This is another sentence.\", \n",
        "        \"And this is a third sentence.\"]\n",
        "\n",
        "# Create the bag of words model\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(data)\n",
        "\n",
        "# Print the vocabulary\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "# Print the bag of words representation of the data\n",
        "print(X.toarray())\n",
        "\n",
        "\n",
        "## and if we use the ngram value then it is more better------------>(n-grams)"
      ],
      "metadata": {
        "id": "wnxLtKVEViXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Toy dataset\n",
        "data = [\"This is a sentence.\", \n",
        "        \"This is another sentence.\", \n",
        "        \"And this is a third sentence.\"]\n",
        "\n",
        "# Create the TF-IDF model\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(data)\n",
        "\n",
        "# Print the vocabulary\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "# Print the TF-IDF representation of the data\n",
        "print(X.toarray())\n"
      ],
      "metadata": {
        "id": "XVWTPdRCX4u7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WORD2VEC**"
      ],
      "metadata": {
        "id": "P-kGUwTPTQrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "# define the corpus\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "\n",
        "# tokenize the documents\n",
        "texts = [[word for word in document.lower().split()] for document in corpus]\n",
        "\n",
        "# train the model\n",
        "model = gensim.models.Word2Vec(texts, size=300, window=5, min_count=1, workers=4)\n",
        "\n",
        "# save the model\n",
        "model.save(\"word2vec.model\")\n",
        "\n",
        "\n",
        "#Once you have trained a Word2vec model, you can use it to perform various tasks such as finding the most similar words to a given word, or calculating the similarity between two words. For example:\n",
        "\n",
        "# load the model\n",
        "model = gensim.models.Word2Vec.load(\"word2vec.model\")\n",
        "\n",
        "# find the most similar words to \"document\"\n",
        "print(model.wv.most_similar(\"document\"))\n",
        "\n",
        "# calculate the similarity between \"document\" and \"first\"\n",
        "print(model.wv.similarity(\"document\", \"first\"))\n"
      ],
      "metadata": {
        "id": "UrY9MMzgbW6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POS STAGGING (PARTS OF SPEECH)**"
      ],
      "metadata": {
        "id": "VprEifSnWaKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to perform POS tagging\n",
        "def pos_tag(text):\n",
        "  # Process the text with spaCy\n",
        "  doc = nlp(text)\n",
        "  \n",
        "  # Iterate over the tokens in the document\n",
        "  for token in doc:\n",
        "    # Print the token and its POS tag\n",
        "    print(token.text, token.pos_)\n",
        "\n",
        "# Example usage\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "pos_tag(text)\n"
      ],
      "metadata": {
        "id": "qbENOF1ybW3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KV82rCwUbWus"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}